{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf24c338-b475-4868-8090-2b43c71b2188",
   "metadata": {},
   "source": [
    "### iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd9e5b3-3a07-4e85-9f47-ab382ff67d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_ftr_ts shape:torch.Size([354, 2]) tr_tgt_ts shape:torch.Size([354])\n",
      "test_ftr_ts shape:torch.Size([152, 2]) test_tgt_ts shape: torch.Size([152])\n",
      "tensor([193,  86, 232, 157,  68, 248, 112, 281, 288, 118, 162,  80, 225, 226,\n",
      "         54, 238, 184,   2,  20, 291, 139, 210, 230, 199, 226, 299, 166,   3,\n",
      "         60, 293])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2780, 0.5250, 0.3784, 0.4917, 0.5662, 0.6930, 0.4871, 0.5133, 0.5725,\n",
       "        0.5281, 0.6049, 0.6848, 0.9075, 0.4949, 0.3451, 0.5739, 0.2705, 0.5880,\n",
       "        0.5405, 0.8055, 0.5857, 0.6551, 0.6603, 0.3754, 0.4949, 0.5499, 0.4681,\n",
       "        0.4263, 0.5509, 0.2935], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "boston_df = pd.read_csv(\"./boston_price.csv\")\n",
    "\n",
    "\n",
    "# 학습과 테스트용 feature와 target 분리. \n",
    "def get_scaled_train_test_feature_target_ts(data_df):\n",
    "    # RM, LSTAT Feature에 Scaling 적용\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features_np = scaler.fit_transform(data_df[['RM', 'LSTAT']])\n",
    "    # 학습 feature, 테스트 feature, 학습 target, test_target으로 분리. \n",
    "    tr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, \n",
    "                                                                          data_df['PRICE'].values, \n",
    "                                                                          test_size=0.3, random_state=2025)\n",
    "    # 학습 feature와 target을 tensor로 변환. \n",
    "    tr_ftr_ts = torch.from_numpy(tr_features)\n",
    "    tr_tgt_ts = torch.from_numpy(tr_target)\n",
    "    test_ftr_ts = torch.from_numpy(test_features)\n",
    "    test_tgt_ts = torch.from_numpy(test_target)\n",
    "    \n",
    "    return tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts\n",
    "\n",
    "tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n",
    "\n",
    "print(f\"tr_ftr_ts shape:{tr_ftr_ts.shape} tr_tgt_ts shape:{tr_tgt_ts.shape}\")\n",
    "print(f\"test_ftr_ts shape:{test_ftr_ts.shape} test_tgt_ts shape: {test_tgt_ts.shape}\")\n",
    "\n",
    "# 방식\n",
    "batch_indexes = torch.randint(0, 300, size=(30,)) # random 30개 추출\n",
    "print(batch_indexes)\n",
    "\n",
    "tr_ftr_ts[batch_indexes, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9989d6a-177a-417f-ac61-4880fb6acc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57750527 0.08967991]\n",
      " [0.5479977  0.2044702 ]\n",
      " [0.6943859  0.06346578]\n",
      " ...\n",
      " [0.65433991 0.10789183]\n",
      " [0.61946733 0.13107064]\n",
      " [0.47307913 0.16970199]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features_np = scaler.fit_transform(boston_df[['RM', 'LSTAT']])\n",
    "\n",
    "print(scaled_features_np)\n",
    "\n",
    "tr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, boston_df['PRICE'].values, \n",
    "                                                                      test_size=0.3, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9d2f20-5a3e-4418-a80f-35cff4767bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n",
    "    # 데이터 건수\n",
    "    N = target_batch.shape[0]\n",
    "    # 예측 값. \n",
    "    predicted_batch = w1 * rm_batch + w2 * lstat_batch + bias\n",
    "    # 실제값과 예측값의 차이\n",
    "    diff_batch = target_batch - predicted_batch \n",
    "    \n",
    "    # weight와 bias를 얼마나 update할 것인지를 계산.  \n",
    "    w1_update = -(2/N) * learning_rate * (torch.matmul(rm_batch, diff_batch))\n",
    "    w2_update = -(2/N) * learning_rate * (torch.matmul(lstat_batch, diff_batch))\n",
    "    bias_update = -(2/N) * learning_rate * torch.sum(diff_batch)\n",
    "    \n",
    "    # weight와 bias가 update되어야 할 값 반환. \n",
    "    return bias_update, w1_update, w2_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50f8f0fe-3091-4dbf-a28c-6b05d7af51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \n",
    "def batch_random_gradient_descent(features, target, iter_epochs=5000, batch_size=30, verbose=True):\n",
    "    # random seed 값 설정. \n",
    "    torch.manual_seed(2025)\n",
    "    w1 = torch.zeros(1, dtype=torch.float32)\n",
    "    w2 = torch.zeros(1, dtype=torch.float32)\n",
    "    bias = torch.ones(1, dtype=torch.float32)\n",
    "    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    rm = features[:, 0]\n",
    "    lstat = features[:, 1]\n",
    "    \n",
    "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
    "    for i in range(1, iter_epochs+1):\n",
    "        # 이렇게 하면 겹치는 부분이 나올 수 있음\n",
    "        batch_indexes = torch.randint(0, target.shape[0], size=(batch_size,))\n",
    "        rm_batch = rm[batch_indexes]\n",
    "        lstat_batch = lstat[batch_indexes]\n",
    "        target_batch = target[batch_indexes]\n",
    "        # Batch GD 기반으로 Weight/Bias의 Update를 구함. \n",
    "        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, \n",
    "                                                                           rm_batch, lstat_batch, \n",
    "                                                                           target_batch, learning_rate)\n",
    "        \n",
    "        # Batch GD로 구한 weight/bias의 update 적용. \n",
    "        w1 = w1 - w1_update\n",
    "        w2 = w2 - w2_update\n",
    "        bias = bias - bias_update\n",
    "        if verbose: # 100회 iteration 시마다 출력\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch: {i}/{iter_epochs}')\n",
    "                # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n",
    "                predicted = w1 * rm + w2*lstat + bias\n",
    "                diff = target - predicted\n",
    "                loss = torch.mean(diff ** 2)\n",
    "                print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n",
    "        \n",
    "    return w1, w2, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904ea840-f66f-48f7-9e10-7e1bdb7499a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 w1, w2, bias: 0.0 0.0 1.0\n",
      "Epoch: 100/5000\n",
      "w1: 9.532495498657227, w2: 1.938775897026062, bias: 15.881488800048828, loss: 78.11859371095792\n",
      "Epoch: 200/5000\n",
      "w1: 11.555778503417969, w2: -0.21187061071395874, bias: 16.998315811157227, loss: 68.93165545311753\n",
      "Epoch: 300/5000\n",
      "w1: 12.718533515930176, w2: -2.56358003616333, bias: 16.81089210510254, loss: 62.173577384915774\n",
      "Epoch: 400/5000\n",
      "w1: 13.90304183959961, w2: -4.585747241973877, bias: 16.800025939941406, loss: 56.79182986963833\n",
      "Epoch: 500/5000\n",
      "w1: 14.967144012451172, w2: -6.40493106842041, bias: 16.75732421875, loss: 52.39103174204248\n",
      "Epoch: 600/5000\n",
      "w1: 15.9356050491333, w2: -8.12226390838623, bias: 16.799663543701172, loss: 48.65957528994758\n",
      "Epoch: 700/5000\n",
      "w1: 16.72344398498535, w2: -9.702632904052734, bias: 16.679140090942383, loss: 45.647002626302246\n",
      "Epoch: 800/5000\n",
      "w1: 17.431224822998047, w2: -11.052210807800293, bias: 16.579404830932617, loss: 43.32511744180553\n",
      "Epoch: 900/5000\n",
      "w1: 18.30120849609375, w2: -12.218840599060059, bias: 16.790555953979492, loss: 41.19309302820594\n",
      "Epoch: 1000/5000\n",
      "w1: 18.896020889282227, w2: -13.377056121826172, bias: 16.724212646484375, loss: 39.51507512617034\n",
      "Epoch: 1100/5000\n",
      "w1: 19.497303009033203, w2: -14.332029342651367, bias: 16.762632369995117, loss: 38.179907030401786\n",
      "Epoch: 1200/5000\n",
      "w1: 20.02489471435547, w2: -15.361384391784668, bias: 16.616548538208008, loss: 36.97981449118922\n",
      "Epoch: 1300/5000\n",
      "w1: 20.65569496154785, w2: -16.16663932800293, bias: 16.70962905883789, loss: 35.959378076769646\n",
      "Epoch: 1400/5000\n",
      "w1: 21.1413516998291, w2: -16.908586502075195, bias: 16.729860305786133, loss: 35.180607353272876\n",
      "Epoch: 1500/5000\n",
      "w1: 21.553415298461914, w2: -17.55459976196289, bias: 16.74453353881836, loss: 34.569505864560895\n",
      "Epoch: 1600/5000\n",
      "w1: 21.698522567749023, w2: -18.186511993408203, bias: 16.61150550842285, loss: 34.14430463349321\n",
      "Epoch: 1700/5000\n",
      "w1: 22.15851593017578, w2: -18.657800674438477, bias: 16.823680877685547, loss: 33.69209562766356\n",
      "Epoch: 1800/5000\n",
      "w1: 22.52720832824707, w2: -19.241857528686523, bias: 16.762527465820312, loss: 33.257829265999305\n",
      "Epoch: 1900/5000\n",
      "w1: 22.855897903442383, w2: -19.710939407348633, bias: 16.727529525756836, loss: 32.939343067118514\n",
      "Epoch: 2000/5000\n",
      "w1: 23.066722869873047, w2: -20.182083129882812, bias: 16.678756713867188, loss: 32.67555355984374\n",
      "Epoch: 2100/5000\n",
      "w1: 23.253768920898438, w2: -20.498327255249023, bias: 16.64407730102539, loss: 32.50706745660395\n",
      "Epoch: 2200/5000\n",
      "w1: 23.58501625061035, w2: -20.8161678314209, bias: 16.72501564025879, loss: 32.33000505453197\n",
      "Epoch: 2300/5000\n",
      "w1: 23.83082389831543, w2: -21.120540618896484, bias: 16.78860092163086, loss: 32.2131939186935\n",
      "Epoch: 2400/5000\n",
      "w1: 23.94977378845215, w2: -21.4285945892334, bias: 16.633108139038086, loss: 32.05312445589569\n",
      "Epoch: 2500/5000\n",
      "w1: 24.06816291809082, w2: -21.6971378326416, bias: 16.649452209472656, loss: 31.95897951785007\n",
      "Epoch: 2600/5000\n",
      "w1: 24.331947326660156, w2: -22.057151794433594, bias: 16.63773536682129, loss: 31.831511859923527\n",
      "Epoch: 2700/5000\n",
      "w1: 24.382705688476562, w2: -22.33140754699707, bias: 16.420347213745117, loss: 31.818175098016848\n",
      "Epoch: 2800/5000\n",
      "w1: 24.668821334838867, w2: -22.400672912597656, bias: 16.683475494384766, loss: 31.739315101552524\n",
      "Epoch: 2900/5000\n",
      "w1: 24.742305755615234, w2: -22.561574935913086, bias: 16.646923065185547, loss: 31.68877565201892\n",
      "Epoch: 3000/5000\n",
      "w1: 24.79235076904297, w2: -22.73005485534668, bias: 16.508460998535156, loss: 31.64541580210268\n",
      "Epoch: 3100/5000\n",
      "w1: 24.866443634033203, w2: -22.746559143066406, bias: 16.579936981201172, loss: 31.635298840242974\n",
      "Epoch: 3200/5000\n",
      "w1: 25.01746940612793, w2: -22.97410011291504, bias: 16.53377342224121, loss: 31.585334301424556\n",
      "Epoch: 3300/5000\n",
      "w1: 25.048316955566406, w2: -23.127559661865234, bias: 16.59482765197754, loss: 31.56495389692983\n",
      "Epoch: 3400/5000\n",
      "w1: 25.10870933532715, w2: -23.22215461730957, bias: 16.50473976135254, loss: 31.54847951438085\n",
      "Epoch: 3500/5000\n",
      "w1: 25.232105255126953, w2: -23.319583892822266, bias: 16.570262908935547, loss: 31.531060791849153\n",
      "Epoch: 3600/5000\n",
      "w1: 25.319252014160156, w2: -23.397397994995117, bias: 16.599708557128906, loss: 31.52657141106204\n",
      "Epoch: 3700/5000\n",
      "w1: 25.34217643737793, w2: -23.519851684570312, bias: 16.500106811523438, loss: 31.502197430597043\n",
      "Epoch: 3800/5000\n",
      "w1: 25.406204223632812, w2: -23.71967887878418, bias: 16.45979118347168, loss: 31.490368260158625\n",
      "Epoch: 3900/5000\n",
      "w1: 25.473514556884766, w2: -23.76553726196289, bias: 16.53398895263672, loss: 31.477908854301642\n",
      "Epoch: 4000/5000\n",
      "w1: 25.5531063079834, w2: -23.864456176757812, bias: 16.5695858001709, loss: 31.472683506747565\n",
      "Epoch: 4100/5000\n",
      "w1: 25.572641372680664, w2: -23.975610733032227, bias: 16.544342041015625, loss: 31.463334569842186\n",
      "Epoch: 4200/5000\n",
      "w1: 25.5407772064209, w2: -24.059768676757812, bias: 16.405149459838867, loss: 31.489693659320668\n",
      "Epoch: 4300/5000\n",
      "w1: 25.717731475830078, w2: -24.045671463012695, bias: 16.52581024169922, loss: 31.458523362924343\n",
      "Epoch: 4400/5000\n",
      "w1: 25.670425415039062, w2: -24.158939361572266, bias: 16.418031692504883, loss: 31.46842270879547\n",
      "Epoch: 4500/5000\n",
      "w1: 25.674362182617188, w2: -24.164718627929688, bias: 16.491012573242188, loss: 31.456175607191526\n",
      "Epoch: 4600/5000\n",
      "w1: 25.817031860351562, w2: -24.170637130737305, bias: 16.583324432373047, loss: 31.46518883851858\n",
      "Epoch: 4700/5000\n",
      "w1: 25.874303817749023, w2: -24.193317413330078, bias: 16.65890884399414, loss: 31.497176078945763\n",
      "Epoch: 4800/5000\n",
      "w1: 25.717987060546875, w2: -24.23040771484375, bias: 16.460956573486328, loss: 31.45707963297639\n",
      "Epoch: 4900/5000\n",
      "w1: 25.86789894104004, w2: -24.19508171081543, bias: 16.586801528930664, loss: 31.469817588047118\n",
      "Epoch: 5000/5000\n",
      "w1: 25.834089279174805, w2: -24.221487045288086, bias: 16.4411678314209, loss: 31.450539146039315\n",
      "##### 최종 w1, w2, bias #######\n",
      "tensor([25.8341]) tensor([-24.2215]) tensor([16.4412])\n"
     ]
    }
   ],
   "source": [
    "# 학습 feature와 target으로 Stochastic Gradient Descent 수행. \n",
    "w1, w2, bias = batch_random_gradient_descent(tr_ftr_ts, tr_tgt_ts, iter_epochs=5000, batch_size=30, verbose=True)\n",
    "print('##### 최종 w1, w2, bias #######')\n",
    "print(w1, w2, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be223c5-c375-49fb-99d4-ec27793a1644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 데이터 세트의 MSE: 28.42650203860407\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED_PRICE_RANDOM_BATCH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.242677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.5</td>\n",
       "      <td>33.231256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.422623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.0</td>\n",
       "      <td>23.116687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.490353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.1</td>\n",
       "      <td>22.167069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.525711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.4</td>\n",
       "      <td>31.063288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.6</td>\n",
       "      <td>31.069676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.9</td>\n",
       "      <td>18.279160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.7</td>\n",
       "      <td>16.139229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.5</td>\n",
       "      <td>25.451039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.937089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.504179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.9</td>\n",
       "      <td>4.853991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.411007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.376532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.1</td>\n",
       "      <td>31.156956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.4</td>\n",
       "      <td>18.550729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.1</td>\n",
       "      <td>23.733847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT  PRICE  PREDICTED_PRICE_RANDOM_BATCH\n",
       "0   0.504311  0.546082   11.0                     16.242677\n",
       "1   0.727534  0.082781   31.5                     33.231256\n",
       "2   0.442422  0.348786   22.0                     19.422623\n",
       "3   0.443380  0.197296   50.0                     23.116687\n",
       "4   0.519640  0.139349   24.1                     26.490353\n",
       "5   0.511401  0.309051   20.1                     22.167069\n",
       "6   0.425752  0.450607   22.5                     16.525711\n",
       "7   0.612569  0.049669   32.4                     31.063288\n",
       "8   0.623683  0.061258   31.6                     31.069676\n",
       "9   0.571757  0.533940   10.9                     18.279160\n",
       "10  0.498755  0.544426   21.7                     16.139229\n",
       "11  0.550297  0.214956   24.5                     25.451039\n",
       "12  0.489366  0.212472   20.5                     23.937089\n",
       "13  0.399885  0.341336   20.8                     18.504179\n",
       "14  0.110558  0.596302   11.9                      4.853991\n",
       "15  0.511209  0.381347   19.4                     20.411007\n",
       "16  0.183560  0.972682    7.0                     -2.376532\n",
       "17  0.704158  0.143488   36.1                     31.156956\n",
       "18  0.625216  0.579746    8.4                     18.550729\n",
       "19  0.548764  0.284216   16.1                     23.733847"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \n",
    "test_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_ftr_ts[:, 1],\n",
    "    'PRICE': test_tgt_ts,\n",
    "    'PREDICTED_PRICE_RANDOM_BATCH': test_predicted_ts.cpu().numpy()\n",
    "})\n",
    "\n",
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_RANDOM_BATCH'])\n",
    "print(\"test 데이터 세트의 MSE:\", test_total_mse)\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bc1b6-7b68-4e84-b196-6fd66a9bf76e",
   "metadata": {},
   "source": [
    "### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fdbddf3-d477-4313-9946-67af740956a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "# 이런 방식으로 30개씩 순차적으로 추출\n",
    "for batch_step in range(0, 506, 30):\n",
    "    print(batch_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad1a3652-97d3-416d-84de-b5b36e40c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \n",
    "def batch_gradient_descent(features, target, epochs=300, batch_size=30, verbose=True):\n",
    "    torch.manual_seed(2025)\n",
    "\n",
    "    w1 = torch.zeros(1, dtype=torch.float32)\n",
    "    w2 = torch.zeros(1, dtype=torch.float32)\n",
    "    bias = torch.ones(1, dtype=torch.float32)\n",
    "    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n",
    "    \n",
    "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
    "    learning_rate = 0.01\n",
    "    rm = features[:, 0]\n",
    "    lstat = features[:, 1]\n",
    "    \n",
    "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
    "    for i in range(1, epochs+1):\n",
    "        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n",
    "        for batch_step in range(0, target.shape[0], batch_size):\n",
    "            # batch_size만큼 순차적인 데이터를 가져옴. \n",
    "            rm_batch = rm[batch_step:batch_step + batch_size]\n",
    "            lstat_batch = lstat[batch_step:batch_step + batch_size]\n",
    "            target_batch = target[batch_step:batch_step + batch_size]\n",
    "        \n",
    "            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, \n",
    "                                                                               rm_batch, lstat_batch, target_batch, \n",
    "                                                                               learning_rate)\n",
    "            # Batch GD로 구한 weight/bias의 update 적용. \n",
    "            w1 = w1 - w1_update\n",
    "            w2 = w2 - w2_update\n",
    "            bias = bias - bias_update\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch: {i}/{epochs}')\n",
    "            # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n",
    "            predicted = w1 * rm + w2*lstat + bias\n",
    "            diff = target - predicted\n",
    "            loss = torch.mean(diff ** 2)\n",
    "            print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n",
    "        \n",
    "    return w1, w2, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9529bfae-2ff2-4759-988b-e31a2770468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 w1, w2, bias: 0.0 0.0 1.0\n",
      "Epoch: 1/300\n",
      "w1: 2.548649311065674, w2: 1.0510945320129395, bias: 5.483735084533691, loss: 324.92663814145334\n",
      "Epoch: 2/300\n",
      "w1: 4.426955699920654, w2: 1.7139111757278442, bias: 8.697149276733398, loss: 206.7792344029648\n",
      "Epoch: 3/300\n",
      "w1: 5.824299335479736, w2: 2.0995075702667236, bias: 11.000228881835938, loss: 145.65487506900436\n",
      "Epoch: 4/300\n",
      "w1: 6.876403331756592, w2: 2.2874581813812256, bias: 12.650941848754883, loss: 113.7959164674643\n",
      "Epoch: 5/300\n",
      "w1: 7.680532455444336, w2: 2.3347744941711426, bias: 13.834155082702637, loss: 96.95631171099302\n",
      "Epoch: 6/300\n",
      "w1: 8.306395530700684, w2: 2.2823007106781006, bias: 14.682345390319824, loss: 87.82619786802942\n",
      "Epoch: 7/300\n",
      "w1: 8.803953170776367, w2: 2.1592955589294434, bias: 15.290448188781738, loss: 82.65587677962309\n",
      "Epoch: 8/300\n",
      "w1: 9.209012985229492, w2: 1.9867151975631714, bias: 15.726494789123535, loss: 79.52265426532097\n",
      "Epoch: 9/300\n",
      "w1: 9.547245025634766, w2: 1.7795655727386475, bias: 16.03923797607422, loss: 77.44113859278626\n",
      "Epoch: 10/300\n",
      "w1: 9.837047576904297, w2: 1.548589825630188, bias: 16.263614654541016, loss: 75.90665671964658\n",
      "Epoch: 11/300\n",
      "w1: 10.091611862182617, w2: 1.3014757633209229, bias: 16.424659729003906, loss: 74.66093404836336\n",
      "Epoch: 12/300\n",
      "w1: 10.32039737701416, w2: 1.0437220335006714, bias: 16.540319442749023, loss: 73.57190174613771\n",
      "Epoch: 13/300\n",
      "w1: 10.530186653137207, w2: 0.7792585492134094, bias: 16.623445510864258, loss: 72.57209131291454\n",
      "Epoch: 14/300\n",
      "w1: 10.725850105285645, w2: 0.5108909606933594, bias: 16.68325424194336, loss: 71.6270248557248\n",
      "Epoch: 15/300\n",
      "w1: 10.910879135131836, w2: 0.24062007665634155, bias: 16.72634506225586, loss: 70.71907466992612\n",
      "Epoch: 16/300\n",
      "w1: 11.087787628173828, w2: -0.030130643397569656, bias: 16.7574520111084, loss: 69.83916732318556\n",
      "Epoch: 17/300\n",
      "w1: 11.258377075195312, w2: -0.30035117268562317, bias: 16.77996826171875, loss: 68.98258518523681\n",
      "Epoch: 18/300\n",
      "w1: 11.423945426940918, w2: -0.5693276524543762, bias: 16.79631996154785, loss: 68.14680103112424\n",
      "Epoch: 19/300\n",
      "w1: 11.585432052612305, w2: -0.8365581631660461, bias: 16.808250427246094, loss: 67.33038325950855\n",
      "Epoch: 20/300\n",
      "w1: 11.743510246276855, w2: -1.1016926765441895, bias: 16.81700897216797, loss: 66.53245714934337\n",
      "Epoch: 21/300\n",
      "w1: 11.898670196533203, w2: -1.364490270614624, bias: 16.823490142822266, loss: 65.75241369049328\n",
      "Epoch: 22/300\n",
      "w1: 12.05126953125, w2: -1.624787449836731, bias: 16.828330993652344, loss: 64.98977580289672\n",
      "Epoch: 23/300\n",
      "w1: 12.201570510864258, w2: -1.8824763298034668, bias: 16.831993103027344, loss: 64.24413100474027\n",
      "Epoch: 24/300\n",
      "w1: 12.349761962890625, w2: -2.1374895572662354, bias: 16.834806442260742, loss: 63.515102723292024\n",
      "Epoch: 25/300\n",
      "w1: 12.495986938476562, w2: -2.389786958694458, bias: 16.837011337280273, loss: 62.8023295369792\n",
      "Epoch: 26/300\n",
      "w1: 12.640355110168457, w2: -2.6393489837646484, bias: 16.838768005371094, loss: 62.105455473727815\n",
      "Epoch: 27/300\n",
      "w1: 12.782946586608887, w2: -2.8861711025238037, bias: 16.84020233154297, loss: 61.424136136474246\n",
      "Epoch: 28/300\n",
      "w1: 12.923829078674316, w2: -3.1302590370178223, bias: 16.841398239135742, loss: 60.75802566345074\n",
      "Epoch: 29/300\n",
      "w1: 13.063050270080566, w2: -3.37162446975708, bias: 16.842422485351562, loss: 60.1067951366227\n",
      "Epoch: 30/300\n",
      "w1: 13.200653076171875, w2: -3.6102852821350098, bias: 16.843318939208984, loss: 59.4701133787079\n",
      "Epoch: 31/300\n",
      "w1: 13.336668968200684, w2: -3.8462624549865723, bias: 16.844120025634766, loss: 58.847661412323106\n",
      "Epoch: 32/300\n",
      "w1: 13.47113037109375, w2: -4.079579830169678, bias: 16.8448429107666, loss: 58.23911857595167\n",
      "Epoch: 33/300\n",
      "w1: 13.604061126708984, w2: -4.310262203216553, bias: 16.845510482788086, loss: 57.64417849669788\n",
      "Epoch: 34/300\n",
      "w1: 13.735486030578613, w2: -4.538336277008057, bias: 16.84613609313965, loss: 57.06253526956494\n",
      "Epoch: 35/300\n",
      "w1: 13.865424156188965, w2: -4.763829231262207, bias: 16.846723556518555, loss: 56.49389330656106\n",
      "Epoch: 36/300\n",
      "w1: 13.993897438049316, w2: -4.98676872253418, bias: 16.84728240966797, loss: 55.937959443987005\n",
      "Epoch: 37/300\n",
      "w1: 14.120925903320312, w2: -5.20718240737915, bias: 16.84781837463379, loss: 55.39444797001354\n",
      "Epoch: 38/300\n",
      "w1: 14.246524810791016, w2: -5.4250969886779785, bias: 16.848325729370117, loss: 54.86308440017538\n",
      "Epoch: 39/300\n",
      "w1: 14.370711326599121, w2: -5.6405415534973145, bias: 16.84881591796875, loss: 54.34359478131271\n",
      "Epoch: 40/300\n",
      "w1: 14.49350357055664, w2: -5.853541851043701, bias: 16.849287033081055, loss: 53.835714540267645\n",
      "Epoch: 41/300\n",
      "w1: 14.614916801452637, w2: -6.064126968383789, bias: 16.849746704101562, loss: 53.3391819257361\n",
      "Epoch: 42/300\n",
      "w1: 14.734968185424805, w2: -6.272323131561279, bias: 16.85018539428711, loss: 52.85374212427559\n",
      "Epoch: 43/300\n",
      "w1: 14.853671073913574, w2: -6.478156566619873, bias: 16.850610733032227, loss: 52.379150426411286\n",
      "Epoch: 44/300\n",
      "w1: 14.97104263305664, w2: -6.681654453277588, bias: 16.851022720336914, loss: 51.91516096838398\n",
      "Epoch: 45/300\n",
      "w1: 15.087099075317383, w2: -6.882843971252441, bias: 16.851421356201172, loss: 51.46153367925099\n",
      "Epoch: 46/300\n",
      "w1: 15.20185375213623, w2: -7.081750869750977, bias: 16.851804733276367, loss: 51.01803815835783\n",
      "Epoch: 47/300\n",
      "w1: 15.315322875976562, w2: -7.278400897979736, bias: 16.8521728515625, loss: 50.584446040549295\n",
      "Epoch: 48/300\n",
      "w1: 15.427520751953125, w2: -7.472818374633789, bias: 16.852529525756836, loss: 50.16053787959218\n",
      "Epoch: 49/300\n",
      "w1: 15.538460731506348, w2: -7.665029048919678, bias: 16.852872848510742, loss: 49.7460965343557\n",
      "Epoch: 50/300\n",
      "w1: 15.648159980773926, w2: -7.855058670043945, bias: 16.853206634521484, loss: 49.34090624646667\n",
      "Epoch: 51/300\n",
      "w1: 15.756628036499023, w2: -8.042930603027344, bias: 16.85352325439453, loss: 48.94476510219093\n",
      "Epoch: 52/300\n",
      "w1: 15.863882064819336, w2: -8.228670120239258, bias: 16.853832244873047, loss: 48.55746683808859\n",
      "Epoch: 53/300\n",
      "w1: 15.96993350982666, w2: -8.412301063537598, bias: 16.854127883911133, loss: 48.17881521199889\n",
      "Epoch: 54/300\n",
      "w1: 16.074796676635742, w2: -8.593847274780273, bias: 16.854413986206055, loss: 47.808616146893904\n",
      "Epoch: 55/300\n",
      "w1: 16.178483963012695, w2: -8.773333549499512, bias: 16.854686737060547, loss: 47.44667891569247\n",
      "Epoch: 56/300\n",
      "w1: 16.281009674072266, w2: -8.950782775878906, bias: 16.854948043823242, loss: 47.092818332764885\n",
      "Epoch: 57/300\n",
      "w1: 16.382389068603516, w2: -9.126214981079102, bias: 16.855194091796875, loss: 46.74685589142862\n",
      "Epoch: 58/300\n",
      "w1: 16.482635498046875, w2: -9.299657821655273, bias: 16.85542869567871, loss: 46.40860742211338\n",
      "Epoch: 59/300\n",
      "w1: 16.581756591796875, w2: -9.471131324768066, bias: 16.85565757751465, loss: 46.077908390459946\n",
      "Epoch: 60/300\n",
      "w1: 16.67976951599121, w2: -9.640658378601074, bias: 16.855871200561523, loss: 45.75458446959982\n",
      "Epoch: 61/300\n",
      "w1: 16.776687622070312, w2: -9.808259963989258, bias: 16.856075286865234, loss: 45.43847175837149\n",
      "Epoch: 62/300\n",
      "w1: 16.87251853942871, w2: -9.973959922790527, bias: 16.856266021728516, loss: 45.12940917675758\n",
      "Epoch: 63/300\n",
      "w1: 16.967275619506836, w2: -10.137777328491211, bias: 16.85645294189453, loss: 44.82724213157652\n",
      "Epoch: 64/300\n",
      "w1: 17.060976028442383, w2: -10.29973316192627, bias: 16.856624603271484, loss: 44.53181187668776\n",
      "Epoch: 65/300\n",
      "w1: 17.153627395629883, w2: -10.459851264953613, bias: 16.856788635253906, loss: 44.24296753639515\n",
      "Epoch: 66/300\n",
      "w1: 17.245243072509766, w2: -10.618151664733887, bias: 16.856943130493164, loss: 43.96056141736629\n",
      "Epoch: 67/300\n",
      "w1: 17.335830688476562, w2: -10.774653434753418, bias: 16.857088088989258, loss: 43.684454308821444\n",
      "Epoch: 68/300\n",
      "w1: 17.42540740966797, w2: -10.929380416870117, bias: 16.857219696044922, loss: 43.414496176395474\n",
      "Epoch: 69/300\n",
      "w1: 17.51398468017578, w2: -11.08234691619873, bias: 16.857341766357422, loss: 43.15055723676509\n",
      "Epoch: 70/300\n",
      "w1: 17.601566314697266, w2: -11.2335786819458, bias: 16.857454299926758, loss: 42.89250038115004\n",
      "Epoch: 71/300\n",
      "w1: 17.688173294067383, w2: -11.383090019226074, bias: 16.857559204101562, loss: 42.64019406444556\n",
      "Epoch: 72/300\n",
      "w1: 17.773807525634766, w2: -11.530904769897461, bias: 16.857656478881836, loss: 42.393510260368515\n",
      "Epoch: 73/300\n",
      "w1: 17.858488082885742, w2: -11.677040100097656, bias: 16.857742309570312, loss: 42.15231981714098\n",
      "Epoch: 74/300\n",
      "w1: 17.942218780517578, w2: -11.821517944335938, bias: 16.857816696166992, loss: 41.916500847982334\n",
      "Epoch: 75/300\n",
      "w1: 18.025014877319336, w2: -11.964353561401367, bias: 16.85788345336914, loss: 41.68593466031366\n",
      "Epoch: 76/300\n",
      "w1: 18.106887817382812, w2: -12.105567932128906, bias: 16.857940673828125, loss: 41.460500041153686\n",
      "Epoch: 77/300\n",
      "w1: 18.18784523010254, w2: -12.245176315307617, bias: 16.857994079589844, loss: 41.24008811460075\n",
      "Epoch: 78/300\n",
      "w1: 18.26789665222168, w2: -12.383198738098145, bias: 16.858036041259766, loss: 41.02458417203075\n",
      "Epoch: 79/300\n",
      "w1: 18.347055435180664, w2: -12.519654273986816, bias: 16.85806655883789, loss: 40.81387504819641\n",
      "Epoch: 80/300\n",
      "w1: 18.425331115722656, w2: -12.654557228088379, bias: 16.858091354370117, loss: 40.607858201159864\n",
      "Epoch: 81/300\n",
      "w1: 18.502731323242188, w2: -12.787928581237793, bias: 16.858112335205078, loss: 40.40642628379284\n",
      "Epoch: 82/300\n",
      "w1: 18.579265594482422, w2: -12.919784545898438, bias: 16.858121871948242, loss: 40.20947763768258\n",
      "Epoch: 83/300\n",
      "w1: 18.65494728088379, w2: -13.050142288208008, bias: 16.858121871948242, loss: 40.01690987134708\n",
      "Epoch: 84/300\n",
      "w1: 18.729782104492188, w2: -13.179017066955566, bias: 16.858112335205078, loss: 39.82862920798283\n",
      "Epoch: 85/300\n",
      "w1: 18.803781509399414, w2: -13.306428909301758, bias: 16.858097076416016, loss: 39.64453527766278\n",
      "Epoch: 86/300\n",
      "w1: 18.876956939697266, w2: -13.432391166687012, bias: 16.858070373535156, loss: 39.46453632627726\n",
      "Epoch: 87/300\n",
      "w1: 18.949316024780273, w2: -13.556922912597656, bias: 16.8580379486084, loss: 39.28853926104879\n",
      "Epoch: 88/300\n",
      "w1: 19.0208683013916, w2: -13.680038452148438, bias: 16.857999801635742, loss: 39.116456401072284\n",
      "Epoch: 89/300\n",
      "w1: 19.09161949157715, w2: -13.801755905151367, bias: 16.85795021057129, loss: 38.94819951526458\n",
      "Epoch: 90/300\n",
      "w1: 19.161582946777344, w2: -13.922090530395508, bias: 16.85789680480957, loss: 38.78368172672798\n",
      "Epoch: 91/300\n",
      "w1: 19.230770111083984, w2: -14.041057586669922, bias: 16.85783576965332, loss: 38.62281816638957\n",
      "Epoch: 92/300\n",
      "w1: 19.299184799194336, w2: -14.158669471740723, bias: 16.85776710510254, loss: 38.465532998795624\n",
      "Epoch: 93/300\n",
      "w1: 19.366836547851562, w2: -14.274946212768555, bias: 16.857688903808594, loss: 38.31174086814142\n",
      "Epoch: 94/300\n",
      "w1: 19.43372917175293, w2: -14.389901161193848, bias: 16.857608795166016, loss: 38.161368552845865\n",
      "Epoch: 95/300\n",
      "w1: 19.4998779296875, w2: -14.503548622131348, bias: 16.857521057128906, loss: 38.014336235439224\n",
      "Epoch: 96/300\n",
      "w1: 19.565290451049805, w2: -14.615903854370117, bias: 16.857425689697266, loss: 37.870569210918084\n",
      "Epoch: 97/300\n",
      "w1: 19.629974365234375, w2: -14.726981163024902, bias: 16.857324600219727, loss: 37.7299952844269\n",
      "Epoch: 98/300\n",
      "w1: 19.693937301635742, w2: -14.836796760559082, bias: 16.857215881347656, loss: 37.59254159506451\n",
      "Epoch: 99/300\n",
      "w1: 19.75718879699707, w2: -14.945364952087402, bias: 16.857101440429688, loss: 37.458137822187204\n",
      "Epoch: 100/300\n",
      "w1: 19.819730758666992, w2: -15.052696228027344, bias: 16.856979370117188, loss: 37.32672213748886\n",
      "Epoch: 101/300\n",
      "w1: 19.88157844543457, w2: -15.158809661865234, bias: 16.856847763061523, loss: 37.198219291263506\n",
      "Epoch: 102/300\n",
      "w1: 19.942737579345703, w2: -15.263714790344238, bias: 16.85671043395996, loss: 37.07256940954945\n",
      "Epoch: 103/300\n",
      "w1: 20.003215789794922, w2: -15.367424964904785, bias: 16.856571197509766, loss: 36.94970933413601\n",
      "Epoch: 104/300\n",
      "w1: 20.063024520874023, w2: -15.46995735168457, bias: 16.85642433166504, loss: 36.82957128871753\n",
      "Epoch: 105/300\n",
      "w1: 20.12216567993164, w2: -15.57132339477539, bias: 16.85626983642578, loss: 36.71209886323102\n",
      "Epoch: 106/300\n",
      "w1: 20.180648803710938, w2: -15.671536445617676, bias: 16.856109619140625, loss: 36.59723154574861\n",
      "Epoch: 107/300\n",
      "w1: 20.238479614257812, w2: -15.770609855651855, bias: 16.855947494506836, loss: 36.48491193396707\n",
      "Epoch: 108/300\n",
      "w1: 20.29566764831543, w2: -15.868555068969727, bias: 16.855775833129883, loss: 36.3750828443045\n",
      "Epoch: 109/300\n",
      "w1: 20.352218627929688, w2: -15.965386390686035, bias: 16.85559844970703, loss: 36.26768829693978\n",
      "Epoch: 110/300\n",
      "w1: 20.40814208984375, w2: -16.061113357543945, bias: 16.855419158935547, loss: 36.16267566941129\n",
      "Epoch: 111/300\n",
      "w1: 20.463443756103516, w2: -16.15575408935547, bias: 16.85523223876953, loss: 36.05998735855757\n",
      "Epoch: 112/300\n",
      "w1: 20.51812744140625, w2: -16.24931526184082, bias: 16.855037689208984, loss: 35.959577478375216\n",
      "Epoch: 113/300\n",
      "w1: 20.57220458984375, w2: -16.34181785583496, bias: 16.854841232299805, loss: 35.861386491415224\n",
      "Epoch: 114/300\n",
      "w1: 20.625680923461914, w2: -16.433263778686523, bias: 16.854639053344727, loss: 35.76537320924731\n",
      "Epoch: 115/300\n",
      "w1: 20.678560256958008, w2: -16.523672103881836, bias: 16.854429244995117, loss: 35.67148450176012\n",
      "Epoch: 116/300\n",
      "w1: 20.73085594177246, w2: -16.613046646118164, bias: 16.854217529296875, loss: 35.57967673116419\n",
      "Epoch: 117/300\n",
      "w1: 20.782567977905273, w2: -16.701404571533203, bias: 16.85399627685547, loss: 35.48990144815517\n",
      "Epoch: 118/300\n",
      "w1: 20.83370590209961, w2: -16.78875732421875, bias: 16.853771209716797, loss: 35.40211248718913\n",
      "Epoch: 119/300\n",
      "w1: 20.884275436401367, w2: -16.875118255615234, bias: 16.853540420532227, loss: 35.31626423786861\n",
      "Epoch: 120/300\n",
      "w1: 20.93429183959961, w2: -16.960493087768555, bias: 16.85330581665039, loss: 35.23231410590175\n",
      "Epoch: 121/300\n",
      "w1: 20.983745574951172, w2: -17.044897079467773, bias: 16.85306739807129, loss: 35.15022300876471\n",
      "Epoch: 122/300\n",
      "w1: 21.032655715942383, w2: -17.128339767456055, bias: 16.852821350097656, loss: 35.06994525652214\n",
      "Epoch: 123/300\n",
      "w1: 21.081016540527344, w2: -17.210834503173828, bias: 16.85257339477539, loss: 34.99144361969015\n",
      "Epoch: 124/300\n",
      "w1: 21.128847122192383, w2: -17.292388916015625, bias: 16.852319717407227, loss: 34.9146754954683\n",
      "Epoch: 125/300\n",
      "w1: 21.1761417388916, w2: -17.37301254272461, bias: 16.852062225341797, loss: 34.83960759425354\n",
      "Epoch: 126/300\n",
      "w1: 21.22291374206543, w2: -17.45271873474121, bias: 16.85179901123047, loss: 34.76619711037601\n",
      "Epoch: 127/300\n",
      "w1: 21.269168853759766, w2: -17.531518936157227, bias: 16.851531982421875, loss: 34.6944065703867\n",
      "Epoch: 128/300\n",
      "w1: 21.31490707397461, w2: -17.609420776367188, bias: 16.85126304626465, loss: 34.62420394335712\n",
      "Epoch: 129/300\n",
      "w1: 21.360139846801758, w2: -17.68643569946289, bias: 16.85098648071289, loss: 34.55555053592399\n",
      "Epoch: 130/300\n",
      "w1: 21.404869079589844, w2: -17.762575149536133, bias: 16.8507080078125, loss: 34.4884126135374\n",
      "Epoch: 131/300\n",
      "w1: 21.449106216430664, w2: -17.83784294128418, bias: 16.850427627563477, loss: 34.422758387329374\n",
      "Epoch: 132/300\n",
      "w1: 21.492855072021484, w2: -17.912256240844727, bias: 16.85013771057129, loss: 34.358550133322396\n",
      "Epoch: 133/300\n",
      "w1: 21.536113739013672, w2: -17.98581886291504, bias: 16.8498477935791, loss: 34.295762622550654\n",
      "Epoch: 134/300\n",
      "w1: 21.578895568847656, w2: -18.058549880981445, bias: 16.84955406188965, loss: 34.23435518017485\n",
      "Epoch: 135/300\n",
      "w1: 21.621204376220703, w2: -18.130447387695312, bias: 16.849254608154297, loss: 34.17430472453892\n",
      "Epoch: 136/300\n",
      "w1: 21.663042068481445, w2: -18.201526641845703, bias: 16.848949432373047, loss: 34.115578478620655\n",
      "Epoch: 137/300\n",
      "w1: 21.70442008972168, w2: -18.271793365478516, bias: 16.84864616394043, loss: 34.05814756943586\n",
      "Epoch: 138/300\n",
      "w1: 21.745342254638672, w2: -18.341262817382812, bias: 16.84833526611328, loss: 34.00197930407412\n",
      "Epoch: 139/300\n",
      "w1: 21.785808563232422, w2: -18.409936904907227, bias: 16.848020553588867, loss: 33.947051824854306\n",
      "Epoch: 140/300\n",
      "w1: 21.825830459594727, w2: -18.47783088684082, bias: 16.84770393371582, loss: 33.893331796391074\n",
      "Epoch: 141/300\n",
      "w1: 21.865407943725586, w2: -18.544954299926758, bias: 16.847381591796875, loss: 33.84079333160132\n",
      "Epoch: 142/300\n",
      "w1: 21.90454864501953, w2: -18.611305236816406, bias: 16.847055435180664, loss: 33.7894156743766\n",
      "Epoch: 143/300\n",
      "w1: 21.94325828552246, w2: -18.676902770996094, bias: 16.846725463867188, loss: 33.739166677942976\n",
      "Epoch: 144/300\n",
      "w1: 21.981534957885742, w2: -18.741750717163086, bias: 16.846389770507812, loss: 33.6900259283809\n",
      "Epoch: 145/300\n",
      "w1: 22.01938819885254, w2: -18.805862426757812, bias: 16.84605598449707, loss: 33.6419650056908\n",
      "Epoch: 146/300\n",
      "w1: 22.056827545166016, w2: -18.86924171447754, bias: 16.845718383789062, loss: 33.59496068651774\n",
      "Epoch: 147/300\n",
      "w1: 22.093852996826172, w2: -18.931900024414062, bias: 16.84537696838379, loss: 33.548989111306376\n",
      "Epoch: 148/300\n",
      "w1: 22.130468368530273, w2: -18.993839263916016, bias: 16.84503173828125, loss: 33.504031006999966\n",
      "Epoch: 149/300\n",
      "w1: 22.16667938232422, w2: -19.05507469177246, bias: 16.844680786132812, loss: 33.460059478024675\n",
      "Epoch: 150/300\n",
      "w1: 22.20249366760254, w2: -19.115615844726562, bias: 16.844329833984375, loss: 33.417051235376604\n",
      "Epoch: 151/300\n",
      "w1: 22.237911224365234, w2: -19.175460815429688, bias: 16.843975067138672, loss: 33.374991155558114\n",
      "Epoch: 152/300\n",
      "w1: 22.27293586730957, w2: -19.23462677001953, bias: 16.843616485595703, loss: 33.33385339246648\n",
      "Epoch: 153/300\n",
      "w1: 22.307580947875977, w2: -19.293113708496094, bias: 16.8432559967041, loss: 33.29361911190465\n",
      "Epoch: 154/300\n",
      "w1: 22.341838836669922, w2: -19.350934982299805, bias: 16.842891693115234, loss: 33.25426846575725\n",
      "Epoch: 155/300\n",
      "w1: 22.37571907043457, w2: -19.408098220825195, bias: 16.8425235748291, loss: 33.21578035617637\n",
      "Epoch: 156/300\n",
      "w1: 22.40922737121582, w2: -19.464603424072266, bias: 16.842153549194336, loss: 33.17813910796835\n",
      "Epoch: 157/300\n",
      "w1: 22.442367553710938, w2: -19.52046775817871, bias: 16.841785430908203, loss: 33.14132134372176\n",
      "Epoch: 158/300\n",
      "w1: 22.475141525268555, w2: -19.57569694519043, bias: 16.841407775878906, loss: 33.105309491046874\n",
      "Epoch: 159/300\n",
      "w1: 22.50755500793457, w2: -19.630294799804688, bias: 16.841026306152344, loss: 33.07008723424718\n",
      "Epoch: 160/300\n",
      "w1: 22.53961181640625, w2: -19.684267044067383, bias: 16.840646743774414, loss: 33.03563811981119\n",
      "Epoch: 161/300\n",
      "w1: 22.57131576538086, w2: -19.737625122070312, bias: 16.84026336669922, loss: 33.001942598349814\n",
      "Epoch: 162/300\n",
      "w1: 22.60266876220703, w2: -19.79037094116211, bias: 16.83987808227539, loss: 32.96898697216578\n",
      "Epoch: 163/300\n",
      "w1: 22.633676528930664, w2: -19.84251594543457, bias: 16.83949089050293, loss: 32.93675223236346\n",
      "Epoch: 164/300\n",
      "w1: 22.66434669494629, w2: -19.89406394958496, bias: 16.839101791381836, loss: 32.90522285965766\n",
      "Epoch: 165/300\n",
      "w1: 22.694677352905273, w2: -19.945024490356445, bias: 16.838706970214844, loss: 32.874383097542434\n",
      "Epoch: 166/300\n",
      "w1: 22.724674224853516, w2: -19.995399475097656, bias: 16.83831024169922, loss: 32.844219557879455\n",
      "Epoch: 167/300\n",
      "w1: 22.754343032836914, w2: -20.045202255249023, bias: 16.837909698486328, loss: 32.81471362170165\n",
      "Epoch: 168/300\n",
      "w1: 22.7836856842041, w2: -20.094432830810547, bias: 16.837509155273438, loss: 32.785854251723016\n",
      "Epoch: 169/300\n",
      "w1: 22.812702178955078, w2: -20.14310646057129, bias: 16.837106704711914, loss: 32.757623973649\n",
      "Epoch: 170/300\n",
      "w1: 22.84140396118164, w2: -20.19122314453125, bias: 16.836702346801758, loss: 32.73000982603587\n",
      "Epoch: 171/300\n",
      "w1: 22.869792938232422, w2: -20.238788604736328, bias: 16.8362979888916, loss: 32.70299889547223\n",
      "Epoch: 172/300\n",
      "w1: 22.897869110107422, w2: -20.285810470581055, bias: 16.835887908935547, loss: 32.67657768381835\n",
      "Epoch: 173/300\n",
      "w1: 22.92563247680664, w2: -20.33229637145996, bias: 16.83547592163086, loss: 32.65073349328594\n",
      "Epoch: 174/300\n",
      "w1: 22.953096389770508, w2: -20.378250122070312, bias: 16.835063934326172, loss: 32.625452422459226\n",
      "Epoch: 175/300\n",
      "w1: 22.980257034301758, w2: -20.42367935180664, bias: 16.83464813232422, loss: 32.60072269528201\n",
      "Epoch: 176/300\n",
      "w1: 23.00712013244629, w2: -20.46858787536621, bias: 16.834232330322266, loss: 32.576532684047685\n",
      "Epoch: 177/300\n",
      "w1: 23.03369140625, w2: -20.512985229492188, bias: 16.833812713623047, loss: 32.55286824190575\n",
      "Epoch: 178/300\n",
      "w1: 23.059965133666992, w2: -20.556875228881836, bias: 16.83339500427246, loss: 32.529720761936666\n",
      "Epoch: 179/300\n",
      "w1: 23.085956573486328, w2: -20.600257873535156, bias: 16.832971572875977, loss: 32.50707814120929\n",
      "Epoch: 180/300\n",
      "w1: 23.111658096313477, w2: -20.64314842224121, bias: 16.832550048828125, loss: 32.48492811822558\n",
      "Epoch: 181/300\n",
      "w1: 23.137081146240234, w2: -20.685546875, bias: 16.832122802734375, loss: 32.463259866540035\n",
      "Epoch: 182/300\n",
      "w1: 23.16222381591797, w2: -20.727458953857422, bias: 16.83169174194336, loss: 32.44206378596546\n",
      "Epoch: 183/300\n",
      "w1: 23.18709945678711, w2: -20.768890380859375, bias: 16.831260681152344, loss: 32.42132752390696\n",
      "Epoch: 184/300\n",
      "w1: 23.211698532104492, w2: -20.80984878540039, bias: 16.83082389831543, loss: 32.40104225627138\n",
      "Epoch: 185/300\n",
      "w1: 23.236026763916016, w2: -20.850337982177734, bias: 16.830394744873047, loss: 32.381198813320914\n",
      "Epoch: 186/300\n",
      "w1: 23.260089874267578, w2: -20.890363693237305, bias: 16.829957962036133, loss: 32.36178590347404\n",
      "Epoch: 187/300\n",
      "w1: 23.283893585205078, w2: -20.929931640625, bias: 16.829519271850586, loss: 32.34279363817036\n",
      "Epoch: 188/300\n",
      "w1: 23.307437896728516, w2: -20.96904754638672, bias: 16.82908058166504, loss: 32.32421320024154\n",
      "Epoch: 189/300\n",
      "w1: 23.330720901489258, w2: -21.00771713256836, bias: 16.82863998413086, loss: 32.30603607517993\n",
      "Epoch: 190/300\n",
      "w1: 23.353748321533203, w2: -21.045942306518555, bias: 16.828197479248047, loss: 32.288253943424046\n",
      "Epoch: 191/300\n",
      "w1: 23.376529693603516, w2: -21.08373260498047, bias: 16.8277530670166, loss: 32.27085528825707\n",
      "Epoch: 192/300\n",
      "w1: 23.399059295654297, w2: -21.12108612060547, bias: 16.827308654785156, loss: 32.25383566513801\n",
      "Epoch: 193/300\n",
      "w1: 23.42134666442871, w2: -21.15801239013672, bias: 16.826862335205078, loss: 32.23718380736922\n",
      "Epoch: 194/300\n",
      "w1: 23.443387985229492, w2: -21.194517135620117, bias: 16.826414108276367, loss: 32.22089246444246\n",
      "Epoch: 195/300\n",
      "w1: 23.465190887451172, w2: -21.230602264404297, bias: 16.825963973999023, loss: 32.2049537920422\n",
      "Epoch: 196/300\n",
      "w1: 23.486753463745117, w2: -21.266273498535156, bias: 16.825511932373047, loss: 32.189360473675855\n",
      "Epoch: 197/300\n",
      "w1: 23.508085250854492, w2: -21.301538467407227, bias: 16.825057983398438, loss: 32.17410275614959\n",
      "Epoch: 198/300\n",
      "w1: 23.52918243408203, w2: -21.336397171020508, bias: 16.82460594177246, loss: 32.15917604593554\n",
      "Epoch: 199/300\n",
      "w1: 23.550050735473633, w2: -21.37085723876953, bias: 16.82415199279785, loss: 32.14457133339549\n",
      "Epoch: 200/300\n",
      "w1: 23.570693969726562, w2: -21.40492057800293, bias: 16.823694229125977, loss: 32.130282046780636\n",
      "Epoch: 201/300\n",
      "w1: 23.591110229492188, w2: -21.438594818115234, bias: 16.8232364654541, loss: 32.11630117549835\n",
      "Epoch: 202/300\n",
      "w1: 23.611309051513672, w2: -21.47187614440918, bias: 16.82277488708496, loss: 32.10262314827236\n",
      "Epoch: 203/300\n",
      "w1: 23.631284713745117, w2: -21.504783630371094, bias: 16.822315216064453, loss: 32.08923838151879\n",
      "Epoch: 204/300\n",
      "w1: 23.651042938232422, w2: -21.537315368652344, bias: 16.821855545043945, loss: 32.07614171487942\n",
      "Epoch: 205/300\n",
      "w1: 23.67058753967285, w2: -21.569469451904297, bias: 16.82139015197754, loss: 32.06332814520466\n",
      "Epoch: 206/300\n",
      "w1: 23.689922332763672, w2: -21.601253509521484, bias: 16.8209285736084, loss: 32.05079071866172\n",
      "Epoch: 207/300\n",
      "w1: 23.709047317504883, w2: -21.632678985595703, bias: 16.820466995239258, loss: 32.038521374110864\n",
      "Epoch: 208/300\n",
      "w1: 23.727962493896484, w2: -21.663742065429688, bias: 16.81999969482422, loss: 32.026516593095565\n",
      "Epoch: 209/300\n",
      "w1: 23.746673583984375, w2: -21.69444465637207, bias: 16.819534301757812, loss: 32.01477108541042\n",
      "Epoch: 210/300\n",
      "w1: 23.765182495117188, w2: -21.72480010986328, bias: 16.819068908691406, loss: 32.00327643345499\n",
      "Epoch: 211/300\n",
      "w1: 23.783491134643555, w2: -21.754806518554688, bias: 16.81859588623047, loss: 31.99202844784442\n",
      "Epoch: 212/300\n",
      "w1: 23.801599502563477, w2: -21.784461975097656, bias: 16.818124771118164, loss: 31.98102418415381\n",
      "Epoch: 213/300\n",
      "w1: 23.81951332092285, w2: -21.813779830932617, bias: 16.817657470703125, loss: 31.970255255056326\n",
      "Epoch: 214/300\n",
      "w1: 23.83723258972168, w2: -21.842763900756836, bias: 16.81718635559082, loss: 31.959716413143607\n",
      "Epoch: 215/300\n",
      "w1: 23.854761123657227, w2: -21.871414184570312, bias: 16.816713333129883, loss: 31.949403521744642\n",
      "Epoch: 216/300\n",
      "w1: 23.87209701538086, w2: -21.899736404418945, bias: 16.816240310668945, loss: 31.939311755036886\n",
      "Epoch: 217/300\n",
      "w1: 23.889244079589844, w2: -21.9277286529541, bias: 16.815765380859375, loss: 31.92943742874689\n",
      "Epoch: 218/300\n",
      "w1: 23.906211853027344, w2: -21.955402374267578, bias: 16.815290451049805, loss: 31.91977281326657\n",
      "Epoch: 219/300\n",
      "w1: 23.92299461364746, w2: -21.982757568359375, bias: 16.814815521240234, loss: 31.910315315802503\n",
      "Epoch: 220/300\n",
      "w1: 23.93959617614746, w2: -22.009798049926758, bias: 16.8143367767334, loss: 31.901059900136378\n",
      "Epoch: 221/300\n",
      "w1: 23.95602035522461, w2: -22.03652572631836, bias: 16.813859939575195, loss: 31.89200263195469\n",
      "Epoch: 222/300\n",
      "w1: 23.97226905822754, w2: -22.062944412231445, bias: 16.813379287719727, loss: 31.88313880149046\n",
      "Epoch: 223/300\n",
      "w1: 23.98834228515625, w2: -22.089061737060547, bias: 16.81290054321289, loss: 31.874463685113096\n",
      "Epoch: 224/300\n",
      "w1: 24.004241943359375, w2: -22.11487579345703, bias: 16.812423706054688, loss: 31.865974493594265\n",
      "Epoch: 225/300\n",
      "w1: 24.01996612548828, w2: -22.140396118164062, bias: 16.811941146850586, loss: 31.857665713984165\n",
      "Epoch: 226/300\n",
      "w1: 24.0355224609375, w2: -22.16562271118164, bias: 16.811458587646484, loss: 31.849533814840726\n",
      "Epoch: 227/300\n",
      "w1: 24.050912857055664, w2: -22.19055938720703, bias: 16.810976028442383, loss: 31.841574808994803\n",
      "Epoch: 228/300\n",
      "w1: 24.066139221191406, w2: -22.215211868286133, bias: 16.81049346923828, loss: 31.83378433252646\n",
      "Epoch: 229/300\n",
      "w1: 24.081201553344727, w2: -22.23957633972168, bias: 16.810012817382812, loss: 31.82616070197833\n",
      "Epoch: 230/300\n",
      "w1: 24.09610366821289, w2: -22.263660430908203, bias: 16.809526443481445, loss: 31.818698590390298\n",
      "Epoch: 231/300\n",
      "w1: 24.110843658447266, w2: -22.28746795654297, bias: 16.809040069580078, loss: 31.81139505576298\n",
      "Epoch: 232/300\n",
      "w1: 24.125425338745117, w2: -22.311002731323242, bias: 16.808557510375977, loss: 31.804246447737018\n",
      "Epoch: 233/300\n",
      "w1: 24.139846801757812, w2: -22.334266662597656, bias: 16.808073043823242, loss: 31.797249778346355\n",
      "Epoch: 234/300\n",
      "w1: 24.154117584228516, w2: -22.35725975036621, bias: 16.80759048461914, loss: 31.79040173966285\n",
      "Epoch: 235/300\n",
      "w1: 24.168235778808594, w2: -22.379987716674805, bias: 16.80710220336914, loss: 31.78369849374057\n",
      "Epoch: 236/300\n",
      "w1: 24.182201385498047, w2: -22.402454376220703, bias: 16.80661392211914, loss: 31.77713718890911\n",
      "Epoch: 237/300\n",
      "w1: 24.19601821899414, w2: -22.424663543701172, bias: 16.806121826171875, loss: 31.770714110713143\n",
      "Epoch: 238/300\n",
      "w1: 24.20969009399414, w2: -22.446613311767578, bias: 16.805635452270508, loss: 31.764427565813833\n",
      "Epoch: 239/300\n",
      "w1: 24.22321319580078, w2: -22.468311309814453, bias: 16.805150985717773, loss: 31.758273952257408\n",
      "Epoch: 240/300\n",
      "w1: 24.23659324645996, w2: -22.489757537841797, bias: 16.804658889770508, loss: 31.75225004953764\n",
      "Epoch: 241/300\n",
      "w1: 24.249826431274414, w2: -22.510957717895508, bias: 16.804168701171875, loss: 31.74635348806093\n",
      "Epoch: 242/300\n",
      "w1: 24.262922286987305, w2: -22.53191566467285, bias: 16.803680419921875, loss: 31.740580463732492\n",
      "Epoch: 243/300\n",
      "w1: 24.275876998901367, w2: -22.552631378173828, bias: 16.803190231323242, loss: 31.734929221222806\n",
      "Epoch: 244/300\n",
      "w1: 24.288692474365234, w2: -22.57310676574707, bias: 16.80270004272461, loss: 31.729397342852685\n",
      "Epoch: 245/300\n",
      "w1: 24.301372528076172, w2: -22.593341827392578, bias: 16.802207946777344, loss: 31.72398245092487\n",
      "Epoch: 246/300\n",
      "w1: 24.313915252685547, w2: -22.61334800720215, bias: 16.801715850830078, loss: 31.718680680275643\n",
      "Epoch: 247/300\n",
      "w1: 24.32632827758789, w2: -22.633121490478516, bias: 16.801225662231445, loss: 31.713490360216785\n",
      "Epoch: 248/300\n",
      "w1: 24.338605880737305, w2: -22.652666091918945, bias: 16.800735473632812, loss: 31.70840943769577\n",
      "Epoch: 249/300\n",
      "w1: 24.350757598876953, w2: -22.671985626220703, bias: 16.800241470336914, loss: 31.703434322928523\n",
      "Epoch: 250/300\n",
      "w1: 24.36277961730957, w2: -22.691083908081055, bias: 16.79974937438965, loss: 31.69856327823768\n",
      "Epoch: 251/300\n",
      "w1: 24.374675750732422, w2: -22.709957122802734, bias: 16.799259185791016, loss: 31.693795039107222\n",
      "Epoch: 252/300\n",
      "w1: 24.38644027709961, w2: -22.728614807128906, bias: 16.798765182495117, loss: 31.689126407936033\n",
      "Epoch: 253/300\n",
      "w1: 24.39808464050293, w2: -22.747053146362305, bias: 16.79827308654785, loss: 31.684555775630496\n",
      "Epoch: 254/300\n",
      "w1: 24.409603118896484, w2: -22.765283584594727, bias: 16.79777717590332, loss: 31.68007969826721\n",
      "Epoch: 255/300\n",
      "w1: 24.420997619628906, w2: -22.783300399780273, bias: 16.79728126525879, loss: 31.67569777552787\n",
      "Epoch: 256/300\n",
      "w1: 24.432273864746094, w2: -22.801105499267578, bias: 16.796789169311523, loss: 31.671407862843633\n",
      "Epoch: 257/300\n",
      "w1: 24.443431854248047, w2: -22.818708419799805, bias: 16.796293258666992, loss: 31.667206399150995\n",
      "Epoch: 258/300\n",
      "w1: 24.4544677734375, w2: -22.83610725402832, bias: 16.795793533325195, loss: 31.663092678221545\n",
      "Epoch: 259/300\n",
      "w1: 24.465391159057617, w2: -22.853307723999023, bias: 16.795297622680664, loss: 31.659063879891193\n",
      "Epoch: 260/300\n",
      "w1: 24.476200103759766, w2: -22.87030601501465, bias: 16.794803619384766, loss: 31.655119423130714\n",
      "Epoch: 261/300\n",
      "w1: 24.486894607543945, w2: -22.88710594177246, bias: 16.794309616088867, loss: 31.651257224175737\n",
      "Epoch: 262/300\n",
      "w1: 24.49747657775879, w2: -22.90371322631836, bias: 16.793813705444336, loss: 31.64747475253712\n",
      "Epoch: 263/300\n",
      "w1: 24.50794219970703, w2: -22.92012596130371, bias: 16.793317794799805, loss: 31.643771505188038\n",
      "Epoch: 264/300\n",
      "w1: 24.518301010131836, w2: -22.936351776123047, bias: 16.79282569885254, loss: 31.64014436798726\n",
      "Epoch: 265/300\n",
      "w1: 24.52855110168457, w2: -22.9523868560791, bias: 16.79233169555664, loss: 31.636592666638975\n",
      "Epoch: 266/300\n",
      "w1: 24.5386905670166, w2: -22.96823501586914, bias: 16.791833877563477, loss: 31.633114642071206\n",
      "Epoch: 267/300\n",
      "w1: 24.548723220825195, w2: -22.983898162841797, bias: 16.791336059570312, loss: 31.629708723384592\n",
      "Epoch: 268/300\n",
      "w1: 24.558652877807617, w2: -22.99938201904297, bias: 16.79084014892578, loss: 31.62637265759133\n",
      "Epoch: 269/300\n",
      "w1: 24.5684757232666, w2: -23.01468849182129, bias: 16.79034423828125, loss: 31.623105258554844\n",
      "Epoch: 270/300\n",
      "w1: 24.578195571899414, w2: -23.029813766479492, bias: 16.78985023498535, loss: 31.61990587994704\n",
      "Epoch: 271/300\n",
      "w1: 24.587814331054688, w2: -23.04476547241211, bias: 16.789356231689453, loss: 31.616772013033692\n",
      "Epoch: 272/300\n",
      "w1: 24.597332000732422, w2: -23.059545516967773, bias: 16.788860321044922, loss: 31.61370225215595\n",
      "Epoch: 273/300\n",
      "w1: 24.606752395629883, w2: -23.07415008544922, bias: 16.788362503051758, loss: 31.610695920256596\n",
      "Epoch: 274/300\n",
      "w1: 24.616071701049805, w2: -23.088586807250977, bias: 16.787864685058594, loss: 31.607751233562457\n",
      "Epoch: 275/300\n",
      "w1: 24.62529754638672, w2: -23.102853775024414, bias: 16.787368774414062, loss: 31.604867079375012\n",
      "Epoch: 276/300\n",
      "w1: 24.634424209594727, w2: -23.116958618164062, bias: 16.786874771118164, loss: 31.60204178229464\n",
      "Epoch: 277/300\n",
      "w1: 24.643455505371094, w2: -23.130895614624023, bias: 16.786378860473633, loss: 31.599274834340026\n",
      "Epoch: 278/300\n",
      "w1: 24.652393341064453, w2: -23.144672393798828, bias: 16.7858829498291, loss: 31.596564160648974\n",
      "Epoch: 279/300\n",
      "w1: 24.661237716674805, w2: -23.158287048339844, bias: 16.785388946533203, loss: 31.593909279587585\n",
      "Epoch: 280/300\n",
      "w1: 24.66999053955078, w2: -23.171743392944336, bias: 16.784893035888672, loss: 31.591308451068233\n",
      "Epoch: 281/300\n",
      "w1: 24.678651809692383, w2: -23.185041427612305, bias: 16.78439712524414, loss: 31.58876094378381\n",
      "Epoch: 282/300\n",
      "w1: 24.68722152709961, w2: -23.198184967041016, bias: 16.783899307250977, loss: 31.586265294201944\n",
      "Epoch: 283/300\n",
      "w1: 24.69569969177246, w2: -23.211177825927734, bias: 16.783401489257812, loss: 31.58382026226085\n",
      "Epoch: 284/300\n",
      "w1: 24.70409393310547, w2: -23.224021911621094, bias: 16.782907485961914, loss: 31.58142450135598\n",
      "Epoch: 285/300\n",
      "w1: 24.712400436401367, w2: -23.23671531677246, bias: 16.782411575317383, loss: 31.579077493910816\n",
      "Epoch: 286/300\n",
      "w1: 24.72062110900879, w2: -23.249258041381836, bias: 16.78192138671875, loss: 31.57677875893537\n",
      "Epoch: 287/300\n",
      "w1: 24.7287540435791, w2: -23.261655807495117, bias: 16.781429290771484, loss: 31.574526510443825\n",
      "Epoch: 288/300\n",
      "w1: 24.736804962158203, w2: -23.273908615112305, bias: 16.78093719482422, loss: 31.57231977721627\n",
      "Epoch: 289/300\n",
      "w1: 24.744773864746094, w2: -23.28601837158203, bias: 16.780441284179688, loss: 31.57015741936608\n",
      "Epoch: 290/300\n",
      "w1: 24.752660751342773, w2: -23.297985076904297, bias: 16.779945373535156, loss: 31.56803895607767\n",
      "Epoch: 291/300\n",
      "w1: 24.76046371459961, w2: -23.309812545776367, bias: 16.779449462890625, loss: 31.5659633311401\n",
      "Epoch: 292/300\n",
      "w1: 24.768184661865234, w2: -23.321502685546875, bias: 16.778953552246094, loss: 31.563929529514798\n",
      "Epoch: 293/300\n",
      "w1: 24.77582550048828, w2: -23.333053588867188, bias: 16.778461456298828, loss: 31.56193724677682\n",
      "Epoch: 294/300\n",
      "w1: 24.783390045166016, w2: -23.344470977783203, bias: 16.77796745300293, loss: 31.559984602275062\n",
      "Epoch: 295/300\n",
      "w1: 24.790882110595703, w2: -23.355756759643555, bias: 16.77747344970703, loss: 31.558070632037502\n",
      "Epoch: 296/300\n",
      "w1: 24.79829216003418, w2: -23.36690902709961, bias: 16.776979446411133, loss: 31.556195553515174\n",
      "Epoch: 297/300\n",
      "w1: 24.805627822875977, w2: -23.377931594848633, bias: 16.776485443115234, loss: 31.554357860273473\n",
      "Epoch: 298/300\n",
      "w1: 24.812889099121094, w2: -23.388826370239258, bias: 16.775991439819336, loss: 31.552556755967466\n",
      "Epoch: 299/300\n",
      "w1: 24.820072174072266, w2: -23.399595260620117, bias: 16.775497436523438, loss: 31.550791671618192\n",
      "Epoch: 300/300\n",
      "w1: 24.82718276977539, w2: -23.410232543945312, bias: 16.775005340576172, loss: 31.549062547960887\n",
      "##### 최종 w1, w2, bias #######\n",
      "tensor([24.8272]) tensor([-23.4102]) tensor([16.7750])\n"
     ]
    }
   ],
   "source": [
    "tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n",
    "\n",
    "# 학습 feature와 target으로 Mini Batch Gradient Descent 수행. \n",
    "w1, w2, bias = batch_gradient_descent(tr_ftr_ts, tr_tgt_ts, epochs=300, batch_size=30, verbose=True)\n",
    "print('##### 최종 w1, w2, bias #######')\n",
    "print(w1, w2, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e401584-2b65-476d-a266-03b4c505660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 데이터 세트의 MSE: 28.330330879942494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED_PRICE_BATCH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.511732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.5</td>\n",
       "      <td>32.899692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.593937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.0</td>\n",
       "      <td>23.164140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.414010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.1</td>\n",
       "      <td>22.236692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.796413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.4</td>\n",
       "      <td>30.820619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.6</td>\n",
       "      <td>30.825219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.9</td>\n",
       "      <td>18.470453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.7</td>\n",
       "      <td>16.412535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.5</td>\n",
       "      <td>25.405163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.950551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.712280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.9</td>\n",
       "      <td>5.560260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.539474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.438430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.1</td>\n",
       "      <td>30.898178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.4</td>\n",
       "      <td>18.725354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.1</td>\n",
       "      <td>23.745702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT  PRICE  PREDICTED_PRICE_BATCH\n",
       "0   0.504311  0.546082   11.0              16.511732\n",
       "1   0.727534  0.082781   31.5              32.899692\n",
       "2   0.442422  0.348786   22.0              19.593937\n",
       "3   0.443380  0.197296   50.0              23.164140\n",
       "4   0.519640  0.139349   24.1              26.414010\n",
       "5   0.511401  0.309051   20.1              22.236692\n",
       "6   0.425752  0.450607   22.5              16.796413\n",
       "7   0.612569  0.049669   32.4              30.820619\n",
       "8   0.623683  0.061258   31.6              30.825219\n",
       "9   0.571757  0.533940   10.9              18.470453\n",
       "10  0.498755  0.544426   21.7              16.412535\n",
       "11  0.550297  0.214956   24.5              25.405163\n",
       "12  0.489366  0.212472   20.5              23.950551\n",
       "13  0.399885  0.341336   20.8              18.712280\n",
       "14  0.110558  0.596302   11.9               5.560260\n",
       "15  0.511209  0.381347   19.4              20.539474\n",
       "16  0.183560  0.972682    7.0              -1.438430\n",
       "17  0.704158  0.143488   36.1              30.898178\n",
       "18  0.625216  0.579746    8.4              18.725354\n",
       "19  0.548764  0.284216   16.1              23.745702"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \n",
    "test_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_ftr_ts[:, 1],\n",
    "    'PRICE': test_tgt_ts,\n",
    "    'PREDICTED_PRICE_BATCH': test_predicted_ts.cpu().numpy()\n",
    "})\n",
    "\n",
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_BATCH'])\n",
    "print(\"test 데이터 세트의 MSE:\", test_total_mse)\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c16a8-4da1-49c5-985c-d4a9f8530339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
