{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248493a9-2519-434d-a715-981795c284a8",
   "metadata": {},
   "source": [
    "1. Forward pass: 입력 → 모델 → 출력 → loss 계산\n",
    "\n",
    "2. Backward pass: loss.backward() → 각 파라미터의 gradient 계산\n",
    "\n",
    "3. Optimizer step: gradient를 기반으로 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2e03ad-4395-4e43-96a0-54b615839948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f67ef20-d038-40ad-83a2-bf61840f141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input\n",
    "input_tensor = torch.tensor([0.2, 0.1], dtype=torch.float64)\n",
    "\n",
    "# Define weights: w1, w2, b1, b2\n",
    "w1 = nn.Embedding(2, 2, dtype=torch.float64)\n",
    "w2 = nn.Embedding(2, 2, dtype=torch.float64)\n",
    "b1 = nn.Embedding(1, 2, dtype=torch.float64)\n",
    "b2 = nn.Embedding(1, 2, dtype=torch.float64)\n",
    "\n",
    "# Init weights: w1, w2, b1, b2\n",
    "w1.weight.data = torch.tensor([[0.2, 0.1], [0.4, 0.15]], dtype=torch.float64, requires_grad=True).t()\n",
    "w2.weight.data = torch.tensor([[0.65, 0.7], [0.45, 0.3]], dtype=torch.float64, requires_grad=True).t()\n",
    "b1.weight.data = torch.tensor([[0.3]], dtype=torch.float64, requires_grad=True).t()\n",
    "b2.weight.data = torch.tensor([[0.5]], dtype=torch.float64, requires_grad=True).t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe6a4f-44de-422d-b51a-03e463efe696",
   "metadata": {},
   "source": [
    "1. W1 임베딩 텐서: [[W1 가중치, W2 가중치], [W3 가중치, W4 가중치]]의 전치 행렬\n",
    "\n",
    "2. W2 임베딩 텐서: [[W5 가중치, W6 가중치], [W7 가중치, W8 가중치]]의 전치 행렬\n",
    "\n",
    "3. b1 임베딩 텐서: [[b1 바이어스]]의 전치 행렬\n",
    "\n",
    "4. b2 임베딩 텐서: [[b2 바이어스]]의 전치 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a031c8-48e2-4759-b037-325d0fe946ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "input_tensor: tensor([0.2000, 0.1000], dtype=torch.float64)\n",
      "******************************\n",
      "w1.weight: Parameter containing:\n",
      "tensor([[0.2000, 0.4000],\n",
      "        [0.1000, 0.1500]], dtype=torch.float64, requires_grad=True)\n",
      "w1.weight.grad: None\n",
      "b1.weight: Parameter containing:\n",
      "tensor([[0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "b1.weight.grad: None\n",
      "******************************\n",
      "w2.weight: Parameter containing:\n",
      "tensor([[0.6500, 0.4500],\n",
      "        [0.7000, 0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "w2.weight.grad: None\n",
      "b2.weight: Parameter containing:\n",
      "tensor([[0.5000]], dtype=torch.float64, requires_grad=True)\n",
      "b2.weight.grad: None\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# Print weights\n",
    "print('*'*30)\n",
    "print('input_tensor:', input_tensor)\n",
    "print('*'*30)\n",
    "print('w1.weight:', w1.weight)\n",
    "# w1.weight.grad = None\n",
    "# If the optimizer.step() method is not called, that is, if backpropagation is not performed, the gradient of the weight is not obtained.\n",
    "print('w1.weight.grad:', w1.weight.grad)\n",
    "print('b1.weight:', b1.weight)\n",
    "print('b1.weight.grad:', b1.weight.grad)\n",
    "print('*'*30)\n",
    "print('w2.weight:', w2.weight)\n",
    "print('w2.weight.grad:', w2.weight.grad)\n",
    "print('b2.weight:', b2.weight)\n",
    "print('b2.weight.grad:', b2.weight.grad)\n",
    "print('*'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf5f598-dc83-447f-beee-20d0d3d874fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer (MLP)\n",
    "net_h1_h2 = torch.matmul(input_tensor, w1.weight) + b1.weight\n",
    "out_h1_h2 = F.sigmoid(net_h1_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40af825b-7e47-4cd9-9d6a-e5b07f8cc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_h1_h2: tensor([[0.3500, 0.3950]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "out_h1_h2: tensor([[0.5866, 0.5975]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "out_h1_h2.grad: None\n"
     ]
    }
   ],
   "source": [
    "# [[net_h1, net_h2]]\n",
    "print('net_h1_h2:', net_h1_h2) \n",
    "# [[out_h1, out_h2]]\n",
    "print('out_h1_h2:', out_h1_h2) \n",
    "print('out_h1_h2.grad:', out_h1_h2.grad) #아직 backword()를호출하지 않은 상태이므로 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacb2121-3db8-442d-afc3-647e0391a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer (MLP)\n",
    "net_o1_o2 = torch.matmul(out_h1_h2, w2.weight) + b2.weight\n",
    "out_o1_o2 = F.sigmoid(net_o1_o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15336c8e-0398-4cc3-b8d5-28c4a973e487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_o1_o2: tensor([[1.2995, 0.9432]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "out_o1_o2: tensor([[0.7858, 0.7198]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "out_o1_o2.grad: None\n"
     ]
    }
   ],
   "source": [
    "# [[net_o1, net_o2]]\n",
    "print('net_o1_o2:', net_o1_o2)\n",
    "# [[out_o1, out_o2]]\n",
    "print('out_o1_o2:', out_o1_o2)\n",
    "print('out_o1_o2.grad:', out_o1_o2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c48f6b1-b8a0-4f1c-a0a4-c5a0b265a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 계산\n",
    "label = torch.tensor([0.99, 0.01], dtype=torch.float64, requires_grad=True) # 정답\n",
    "loss = torch.sum(0.5*torch.square(label - out_o1_o2)) # 1/2(label - loss)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "946b9abd-870d-4e99-b774-8efaf11ee8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.2727, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222658e-82bf-428a-812e-91965d638420",
   "metadata": {},
   "source": [
    "## Backward Pass 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c88fcf6-79c6-4b57-abdf-dc2eb57bd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gradient of each weight & bias\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9af1ce6-839d-45a3-8319-c64e892c3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.weight.grad: tensor([[0.0020, 0.0009],\n",
      "        [0.0010, 0.0005]], dtype=torch.float64)\n",
      "b1.weight.grad: tensor([[0.0147]], dtype=torch.float64)\n",
      "w2.weight.grad: tensor([[-0.0202,  0.0840],\n",
      "        [-0.0205,  0.0855]], dtype=torch.float64)\n",
      "b2.weight.grad: tensor([[0.1088]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Gradients\n",
    "# Save gradients in weight.grad attribute\n",
    "print('w1.weight.grad:', w1.weight.grad) #∂Loss / ∂w\n",
    "print('b1.weight.grad:', b1.weight.grad)\n",
    "print('w2.weight.grad:', w2.weight.grad)\n",
    "print('b2.weight.grad:', b2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de74cfd7-d0bd-4b36-b88e-4da9766c1f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.weight: Parameter containing:\n",
      "tensor([[0.2000, 0.4000],\n",
      "        [0.1000, 0.1500]], dtype=torch.float64, requires_grad=True)\n",
      "b1.weight: Parameter containing:\n",
      "tensor([[0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "w2.weight: Parameter containing:\n",
      "tensor([[0.6500, 0.4500],\n",
      "        [0.7000, 0.3000]], dtype=torch.float64, requires_grad=True)\n",
      "b2.weight: Parameter containing:\n",
      "tensor([[0.5000]], dtype=torch.float64, requires_grad=True)\n",
      "loss: tensor(0.2727, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "label: tensor([0.9900, 0.0100], dtype=torch.float64, requires_grad=True)\n",
      "output: tensor([[0.7858, 0.7198]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Before optimization\n",
    "# Weights\n",
    "print('w1.weight:', w1.weight)\n",
    "print('b1.weight:', b1.weight)\n",
    "print('w2.weight:', w2.weight)\n",
    "print('b2.weight:', b2.weight)\n",
    "# Loss\n",
    "h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "print('loss:', torch.sum(0.5*torch.square(label - output)))\n",
    "# Output\n",
    "print('label:', label)\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a753d5c-df88-4e82-85f7-f7bde0467cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "# Optimizer \n",
    "optimizer = optim.SGD((w1.weight, w2.weight, b1.weight, b2.weight), lr=0.5) # loss.backward()를 먼저 실행해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a03864-8497-4acc-acc2-39681e07cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387496ad-466a-498e-a410-7739db24adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.weight: Parameter containing:\n",
      "tensor([[0.1990, 0.3995],\n",
      "        [0.0995, 0.1498]], dtype=torch.float64, requires_grad=True)\n",
      "b1.weight: Parameter containing:\n",
      "tensor([[0.2926]], dtype=torch.float64, requires_grad=True)\n",
      "w2.weight: Parameter containing:\n",
      "tensor([[0.6601, 0.4080],\n",
      "        [0.7103, 0.2572]], dtype=torch.float64, requires_grad=True)\n",
      "b2.weight: Parameter containing:\n",
      "tensor([[0.4456]], dtype=torch.float64, requires_grad=True)\n",
      "loss: tensor(0.2591, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "label: tensor([0.9900, 0.0100], dtype=torch.float64, requires_grad=True)\n",
      "output: tensor([[0.7781, 0.6979]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Optimization (1 epoch)\n",
    "# Optimizing weights\n",
    "print('w1.weight:', w1.weight)\n",
    "print('b1.weight:', b1.weight)\n",
    "print('w2.weight:', w2.weight)\n",
    "print('b2.weight:', b2.weight)\n",
    "# Decreasing loss\n",
    "h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "print('loss:', torch.sum(0.5*torch.square(label - output)))\n",
    "# More optimizing output\n",
    "print('label:', label)\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d0430-2940-4501-9535-b55b01bdc4ae",
   "metadata": {},
   "source": [
    "* zero_grad() → forward → loss → backward → step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8a6068-d254-45bd-b31d-b13ae6f00753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.2591, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.1431, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0851, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0546, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0373, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0269, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0205, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0162, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0132, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0110, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0094, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0081, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0071, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0063, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0057, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0051, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0047, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0043, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0039, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0036, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0034, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0031, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0029, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0028, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0026, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0024, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0023, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0022, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0021, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0020, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0019, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0018, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0017, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0017, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0016, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0015, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0015, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0014, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0014, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0013, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0013, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0012, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0012, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0011, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0011, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0011, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0010, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0010, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0010, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0009, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0009, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0009, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0009, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0008, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0008, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0008, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0008, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0008, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0007, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0006, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0005, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0004, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "loss: tensor(0.0003, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1000 epochs\n",
    "for i in range(1000):\n",
    "    \n",
    "    # Init gradient of optimizer\n",
    "    # If this method not called, gradient is stacked.\n",
    "    optimizer.zero_grad() # 한 에폭 끝날때마다 저장된 그레디언트를 0으로 만듦\n",
    "    \n",
    "    # Foward pass\n",
    "    h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "    output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "    \n",
    "    # Loss\n",
    "    loss = torch.sum(0.5*torch.square(label - output))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        # Decreasing loss\n",
    "        print('loss:', loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b078f9-d83b-4567-a11c-758c04958f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: tensor([0.9900, 0.0100], dtype=torch.float64, requires_grad=True)\n",
      "output: tensor([[0.9717, 0.0287]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Validation of output (1000 epochs)\n",
    "h1 = F.sigmoid(torch.matmul(input_tensor, w1.weight) + b1.weight)\n",
    "output = F.sigmoid(torch.matmul(h1, w2.weight) + b2.weight)\n",
    "# Output: close to the label\n",
    "print('label:', label)\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ca6f7-9769-43a7-a408-80db07aea49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
